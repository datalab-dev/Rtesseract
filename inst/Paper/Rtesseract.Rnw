\documentclass[article]{jss}
\usepackage{thumbpdf,lmodern}
\usepackage{float}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{comment}


% Things we should add or make certain are there 
%
% + Examples showing setting some of the options and why they matter
%   e.g. language, white list
% + Querying current options/configuration settings.
%
% + Getting the dimension of the image 
%    See  getImage.R 
% + Limiting to a particular  subregion.
%     See ./setRectangle.R and the SetRectangle example.
%
% + Setting an image after processing it in R first. Have to check this works. 
%     SetImage_raw
% 
%     All names mirror the C++ API, so you can find attitional
%     documentation in the Tesseract documentation.


\author{Matthew B. Espe\\UC - Davis
  \And Duncan Temple Lang \\UC - Davis}
\Plainauthor{Matthew B. Espe, Duncan Temple Lang}

\title{\pkg{Rtesseract}: A package for Optical Character Recognition (OCR) from \proglang{R}}
\Plaintitle{}
\Shorttitle{}

\Abstract{

  We created the \pkg{Rtesseract} package to support working with text
  data present in images. \pkg{Rtesseract} is an \proglang{R} package that
  provides access to Tesseract, an Open Source C++ library for Optical
  Character Recognition (OCR), from within \proglang{R}.  At its simplest,
  \pkg{Rtesseract} allows an \proglang{R} user to recover text from an image
  as lines, words or individual characters.  We can also obtain the
  location of text elements, possible alternatives and the predictive
  ``confidence'' for each.  These are often necessary for interpreting
  the text, e.g., for multi-column layout, or tables.  The package
  provides access to much of the Tesseract C++ API, allowing full
  customization of the OCR behavior, and exporting the results in
  various formats (e.g. PDF), and also visualizing them within R
  itself.  With the \pkg{Rtesseract} package, users are able to create
  a fully programmatic workflow to extract text data into \proglang{R} for
  subsequent analysis.  }

\Keywords{}
\Plainkeywords{}
\Address{
  Matthew Espe, Duncan Temple Lang\\
  Data Science Initiative\\
  University of California - Davis\\
  360 Shields Library\\
  Davis, CA 95616 USA\\
  E-mail: \email{mespe@ucdavis.edu}
}

\begin{document}

\pagenumbering{arabic}

\section{Introduction}\label{intro}

<<setup, echo=FALSE, message=FALSE, warning=FALSE>>=
library(Rtesseract)
library(knitr)

options(prompt = "R> ", continue = "+  ", width = 70, useFancyQuotes = FALSE)

# Globally set output to be limited to 6 lines
knit_theme$set("print")
opts_chunk$set(out.lines = 6)
opts_chunk$set(highlight = FALSE, background = "white", prompt = TRUE,
               comment = '', tidy = TRUE)


# # the default output hook
# # From https://github.com/yihui/knitr-examples
hook_output = knit_hooks$get("output")
knit_hooks$set(output = function(x, options) {
    if (!is.null(n <- options$out.lines)) {
        x = unlist(stringr::str_split(x, "\n"))
        # Truncate long output
        x = sapply(x, function(string) {
            tmp = gsub("[[:space:]] +$", "", string)
            if(nchar(tmp) > 70){
                string = substring(string, first = 1L, last = 66L)
                string = paste(string, '..."')
            }
            string
        })
        if (length(x) > n) {
            # truncate the output
            x = c(head(x, n), "....\n")
        }
        x = paste(x, collapse = "\n")  # paste first n lines together
    }
    hook_output(x, options)
})

# Put this in here to avoid issues with it not being defined later
smithburn = system.file("images", "SMITHBURN_1952_p3.png", package = "Rtesseract")          

@ 

There has been an enormous increase in the amount of text data that we
work with from social media, reports, scholarly articles, job
applications, etc. These often come as plain text or structured
documents such as word-processing files, HTML, or PDF. However, there are
many documents of interest that are only available as scanned or
photographic images (e.g., old books, newspapers).
Optical Character Recognition (OCR) is a technique to infer
characters, words or lines of text from an image, thereby making them
accessible for data analysis.

There are many commercial OCR software tools
currently available (e.g., Adobe Acrobat, PDFPen Pro, ABBY, Google Cloud Vision), though these
typically only output text results.
Although OCR technology has steadily improved over the years, OCR
results are rarely perfect. We may not care about these errors if they
are inconsequential to subsequent analysis. However, often these error
impact our analysis and we need to correct them before
preceding. Therefore, OCR is commonly part of a workflow that involves
pre- and post-processing steps to verify and increase the accuracy of
the text extraction.

A typical OCR workflow might first start with image pre-processing. 
The image might be cropped, contrast increased, noise decreased, etc. using image
editing software to make the text ``stand out'' prior to OCR. Then
after OCR has been run we will check if the results make sense. In
some cases there are obvious issues, such as nonsense words or garbled
text, but in other cases we might want to focus our attention on the
areas where the OCR engine had the least certainty. When we have
prior contextual knowledge about the correct matches, such as only
numbers or certain characters or words occur in the text, we can
leverage this information to correct errors in the post-processing
steps. When the text is arranged in columns or blocks, we will want to
rearrange the text into the correct order. Finally, we will analyze
the text results using methods such as NLP. Using OCR software which
only returns text results, our full OCR workflow would involve 1)
multiple platforms to perform pre-processing, OCR, post-processing,
and analysis, 2) using pre- and post-processing steps to overcome
limited control over the OCR process itself, 3) limited options to
programmatically control and reproduce each step in the workflow, and
4) no access to OCR intermediate products, such as confidences,
alternative matches, location on the page, etc., which are helpful for
assessing the certainty in each returned result.

In our work extracting and analyzing text data present in images from
\proglang{R}, we desired a tool which would overcome the limitations above. In
our search for solutions, we found Tesseract, a long-standing open
source OCR library originally created in 1985 and currently under
active development. Accessing the Tesseract API from \proglang{R} provides us
with the ability to bring pre-processing, OCR, post-processing, and
analysis under complete programmatic control within \proglang{R}. The
\pkg{Rtesseract} package provides 1) a simple high-level function to
extract the text from one or more images, 2) the ability to also
obtain important auxiliary data (e.g., the confidence in each
prediction, the location within the image), and 3) customization of
how the OCR is done.

% Use the term certainty/uncertainty

\begin{comment}
In response to our needs, we developed the \pkg{Rtesseract} package,
an interface to the open source OCR library Tesseract. Tesseract was
created 1985, was released for open source in 2005, and has since been
actively maintained and developed by the open source community
(including developers at Google). Recent innovations in Tesseract
include the integration of LSTM Deep Learning networks (versions >=
4.0) for increased speed and accuracy. By interfacing with Tesseract,
the \pkg{Rtesseract} package provides us 1) a simple high-level
function to extract the text from one or more images, 2) the ability
to also obtain important auxiliary data (e.g., the confidence in each
prediction, the location within the image), and 3) customization of
how the OCR is done. Additionally, the results are accessible directly
within \proglang{R} as text and/or a data frame of locations and confidences.
\end{comment}

This approach using \pkg{Rtesseract} tightly integrates pre- and
post-processing steps with the actual OCR, all from within
\proglang{R}.
For example, we can focus
on a sub-region of the image, or enhance the image with some computer
vision techniques before performing the character recognition. We can
specify a domain-specific vocabulary, or identify unconventional
fonts. We can adjust the OCR behavior via configuration variables to
correct errors in the recognition step rather than relying on
post-processing. This tight integration simplifies workflows that
otherwise rely on invoking stand-alone applications typically meant
for human interactive use. Furthermore, metadata about the resulting
text can aid and inform subsequent analyses. For example, the layout
of the text into blocks, the presence of elements such as lines or
other dividers, or even the relative size of the text can all be used
to make sense of the resulting text. Diagnostic pieces of information
such as the certainty in the matches, alternative matches, etc., can
be essential assessing certainty and improving OCR accuracy. If the
OCR tool only returns the text results, as is the case with many OCR
tools, OCR's usefulness is limited to cases where the problem is
either simple or well-covered by the default configuration.

We have not been alone in recognizing the utility of Tesseract. Prior
to our creation of the \pkg{Rtesseract} in April 2015, the \pkg{ocR}
package~\citep{Hagmann2014} provided an interface in \proglang{R} to Tesseract
via calling the Tesseract command line executable. Using the \pkg{ocR}
package, the user specifies the image file and command-line options,
Tesseract is called and writes the results to a file which is then
read back into \proglang{R}. Then, following the creation of our \pkg{Rtesseract},
the \pkg{tesseract} package~\citep{ooms2016} was created and takes a
similar approach to our \pkg{Rtesseract} package by interfacing to the
Tesseract C\texttt{++} library. However, by design, \pkg{tesseract}
provides only functionality to get the text results from the OCR
process. %% Check this\
\pkg{Rtesseract}, from its initial
implementation, provides access to most of the functionality in the
Tesseract API and provides fine-grain control from \proglang{R} to the different
steps in the OCR process. Additionally, \pkg{Rtesseract} provides
tools which allow users to easily extend functionality, as well as
access to leptonica, the image library Tesseract uses for image
pre-processing. This functionality allows users to provide their own
processed image to Tesseract. Lastly, \pkg{Rtesseract} has been
designed to support both the current release (v3.05.01) and
development versions (v4.00.dev) of Tesseract, with mechanisms to
configure \pkg{Rtesseract} according to the installed versions of
Tesseract, leptonica, and image libraries.

\section{Optical Character Recognition using\pkg{Rtesseract}}\label{desc}

For many simple problems, merely extracting text from an image in a
reliable manner is sufficient.  \pkg{Rtesseract} provides simple
interfaces which allow users to provide the path to an image in a
supported format, e.g. tiff or png, and to perform OCR on the image
using the default settings (similar to both the \pkg{ocR} and
\pkg{tesseract} packages). For example, suppose we have an image from
an older scientific publication from which we want to extract
information, e.g., Fig. \ref{fig:base}. We will use this image to
highlight the functionality and common use-cases of the
\pkg{Rtesseract} package. % AWK


\begin{figure}[h]
\includegraphics[width=\textwidth]{\Sexpr{smithburn}}
\caption{An example image of a scientific paper which contains text we
  would like to extract and analyze.}
\label{fig:base}
\end{figure}

\subsection{Basic Text Extraction}

The most fundamental task for OCR is to convert an image into text.
For this simple task, we use the high-level function \code{GetText},

<<>>=
smithburn = system.file("images", "SMITHBURN_1952_p3.png",
                        package = "Rtesseract")
GetText(smithburn)
@

We pass \code{GetText} the path to a file containing the image.  By
default, it returns the words it finds in the image in the order they
appear.  We sometimes want the results returned as individual
characters (symbols), such as when we are checking how well Tesseract
recognized individual characters, e.g. special or unique characters,

<<>>=
GetText(smithburn, level = "symbol")
@ 

or sometimes we only want full lines (textlines),

<<>>=
GetText(smithburn, level = "textline")
@

Modifying the organizational level of the returned text is easy to
achieve with the \code{levels} argument in \code{GetText()}.

\pkg{Rtesseract} includes three other high-level functions --
\code{GetBoxes()}, \code{GetConfidences()}, and \code{GetAlternatives()} --
to extend the information accessible by the user. Similar to
\code{GetText()}, these functions can take the path of the image file
and the optional \code{level} argument (except \code{GetAlternatives()}
which only operates at the ``symbol'' level).

\subsection{Creating a persistent API object}

This practice of calling \code{GetText()} followed by another, e.g.,
\code{GetBoxes()} for the same file is a common occurrence when working
interactively with \pkg{Rtesseract}. Yet, it has a distinct
inefficiency - we repeat the OCR recognition for the same file merely
to export a different aspect of the output from the OCR engine. Since
OCR is reasonably computationally intensive, we would like to perform
the OCR once and then query each aspect we want (i.e., the text, the
confidences, the alternatives, \ldots) from this single OCR operation.

The \code{tesseract()} function creates an instance of the OCR engine
(an object of class \code{TesseractBaseAPI} in \proglang{R}).  We can repeatedly
query this for the text, boxes, confidences and alternatives without
repeating the OCR.  This is the preferred method when multiple pieces
of information are needed from the same image.

We typically call \code{tesseract()} by specifying the path to the file
and any other options controlling how the OCR will be performed
(explained in more detail below in ``Advanced Use''):

<<>>=
api = tesseract(smithburn)
@

We then pass the object returned by \code{tesseract()} to each of the
four high level functions rather than the file name, e.g.,
\code{GetText(api)}, \code{GetBoxes(api)}, \code{GetConfidences(api)}.
By default, the call to \code{tesseract()} doesn't perform the OCR
immediately. Instead, it is only done when we request any results,
e.g., calling any of the four functions above, or explicitly calling
the \code{Recognize()} function with the \code{TesseractBaseAPI} object
as an argument.

We can use this \code{TesseractBaseAPI} object to illustrate how
\pkg{Rtesseract}'s ability to retrieve more than just the recognized
text from an image can extend an OCR workflow. To continue the example
above, after extracting the text from the scientific publication, we
might want to check the results for inaccuracies. However, checking
word by word manually is both tedious and error-prone. Rather, it
seems reasonable to focus our attention initially on those words with
the lowest confidences:

<<>>=
confs = GetConfidences(api)
sort(confs)[1:10]
@ 

Clearly, some of these do not look correct. To further investigate, we
might want to see where these low-confidence words occur on the page
by extracting the bounding boxes,

<<>>=
bbox = GetBoxes(api)
@

\code{GetBoxes()} extends the information returned by
\code{GetConfidences()} by including the left, top, right, and bottom of
the bounding box around the word, symbol, textline, etc. Using these,
we can sort by the confidence,

<<>>=
bbox[order(bbox[,"confidence"])[1:10],]
@

At first glance, it appears the low confidence words might be grouped
together in similar areas on the page, with the far right side of the
page (``left'' and ``right'' values of approximately 3800-4000)
showing up often in the lowest confidence results.

Since we have access to the confidences and bounding boxes in \proglang{R}, we
can explore this theory by treating these objects just like any other
data. To explore our hypothesis that these terms are grouped left to
right, we can group the words into bins by their location left to
right using the \code{cut()} function,

<<>>=
lrGroups = cut(bbox[,"left"],
               breaks = seq(min(bbox[,"left"]),
                            max(bbox[,"left"]),
                            length.out = 11))
               
@ 

And then calculate the median confidence inside each group,

<<out.lines=8>>=
tapply(bbox[,"confidence"], lrGroups, median)
@

Doing this verifies our suspicion that there are issues with the right
side of the page. Looking at the image (Fig. \ref{fig:base}), we can
see there is rotated text in the margin that identifies when and where
the PDF was downloaded. This text is not being recognized well by the
OCR, but is also not part of the journal article in which we are
interested. Because this text crosses several textlines,
post-processing without location information would involve searching
for ``junk'', e.g. non-valid words such as ``Isosziim'', at the end of
each text line. However, we can easily exclude this text in
post-processing with location information in hand. Additionally, while
we could crop this image using editing software outside of \proglang{R}, we will
show later how this area can be excluded from the OCR recognition step
using \pkg{Rtesseract} without leaving \proglang{R}.

%% Not a great example
To continue, suppose we also wanted to extract the author's
name. Using some prior knowledge that this information is present at
the top of each page, we can focus on the first text line,

<<>>=
GetText(api, level = "textline")[1]
@ 

We know that the lower case letter in abbreviated first and middle
names is a mistake since the letter must be upper case.
But we might not be sure what the correct letter should be.
Should it be an ``O'', or a ``C''?
Helpfully, we can inspect the alternative matches, in this case for
the sixth character in the OCR output,

<<>>=
GetAlternatives(api)[6]
@ 

which reveals that ``C'' was a close second in this case. The
alternatives which are capital letters are ``O'' and ``G'', and both have
substantially lower confidences than ``C''. Therefore, we can be reasonably
certain that the author's name should be ``K. C. Smithburn'' without
inspecting the original image.

This example illustrates both the workflow of using OCR to extract
text and highlights why having access to information beyond just the
text is so helpful to a full OCR workflow, if not crucial for increased
accuracy and error checking.

\subsection{Visualizing OCR results}

In this OCR workflow, we find that visualizing the confidences is
helpful for quickly inspecting \pkg{Rtesseract} results for
issues. Therefore, \pkg{Rtesseract} includes a plotting method for OCR
output. By default, calling \code{plot()} on a \code{TesseractBaseAPI}
object will create a plot which includes original image overlaid with
the bounding boxes colored (and optionally filled) by relative
confidence. See Fig. \ref{fig:rtess_plot} for an example. Plotting the
results is especially helpful when adjusting the OCR configuration
settings to improve accuracy (discussed further below).

\begin{figure}[H]
<<echo = FALSE>>=
plot(api, fillBoxes = TRUE, main = "")
@
\caption{\pkg{Rtesseract}'s plotting function allows the user to
  overlay the scanned image with the bounding boxes surrounding the
  recognized element optionally colored according to the boxes'
  associated confidences.}
\label{fig:rtess_plot}
\end{figure}

\subsection{Zooming and subsetting an image}

It is sometimes useful to ``zoom in'' on a part of the image to inspect a region that is not being accurately recognized.
For visualizing a sub-area of the original image, we use the \code{plotSubImage()} function.
For example, we can find the lowest confidence word,

<<>>=
bbox = GetBoxes(api)
i = which.min(bbox$confidence)
bbox[i,]
@

and then we can zoom in on that word,

\begin{figure}[H]
<<>>=
plotSubImage(as.matrix(bbox[i,1:4]), img = png::readPNG(smithburn))
@ 
\caption{An example of plotting a zoomed in segment of an image using \pkg{Rtesseract}}
\label{fig:zoom}
\end{figure}

As before, we can see the text is rotated for this character, resulting in
poor OCR results.

There are times when there is only a part of the image that we want to recognize with OCR.
In this case, we may want to avoid the computations involved in recognizing the entire contents of an image when we are only interested a piece of it.
We could crop the original image, create a new file containing only the area of interest, and then call \code{tesseract()} on this image.
However, it would be better to focus only on the area of interest without modifying the original image.
For this, \pkg{Rtesseract} includes the \code{SetRectangle()} function. For example, if we know we are only interested in the table on the lower half of the page, we can recognize only this area. First, we can locate where ``TABLE'' is,

<<>>=
bbox[grep("TABLE", bbox$text),]
@ 

Then we reduce the recognization rectangle to only this area using the \code{SetRectangle()}function,
with the dimensions specified as left edge, top edge, width of box, and height of box 

<<>>=
SetRectangle(api, dims = c(0, 1864, 4000, 6000 - 1864))
GetBoxes(api)
@

Although this example only highlights basic uses of the ability to restrict the recognized area, we will show later how we can use \code{SetRectangle()} in conjunction with image manipulation to handle more complex tasks.
Setting the recognition area can be helpful in cases where there are
multiple languages in the same image, or when one area is most
accurately recognized with settings suboptimal for other areas. 

\section{Controlling the OCR engine behavior}

Thus far, we have focused on functions in \pkg{Rtesseract} which allow
accessing and manipulating the results of running the OCR engine on an
image. In many non-trivial cases, you might want to manipulate the
behavior of the OCR engine itself rather than only the output. For
these purposes, the \pkg{Rtesseract} package exposes full access to
the \code{TesseractBaseAPI}, which allows fine-tuning of the OCR
engine for specific cases.

\subsection{Page Segmentation Mode and OCR engine}

There are two ``meta-variables'' control how that Tesseract recognizes the arrangement of text and which OCR engine is used,
\code{pageSegMode} and \code{engineMode}. Since adjusting these two
variables can have a large impact on the results of the OCR, they
deserve special mention.

The first, the Page Segmentation Mode,
adjusts how Tesseract recognizes the layout of the text in the image.
Correctly identifying the text layout is an important first step to accurate OCR.
As we have seen previously, text cannot be properly recognized when it is arranged vertically where Tesseract expects horizontal layout.
Adjusting the Page Segmentation Mode variable allows us to directly tell Tesseract how to
expect the text to be arranged, e.g. as a single block of text, sparse
text, arranged top to bottom (e.g., the vertical arrangement of many
Asian texts) or even in a circle (e.g., the text on the rim of a
coin), etc.
Included in the available options are to have Tesseract
attempt to automatically detect the text orientation, the script used,
or both. %more here
The \pkg{Rtesseract} package preserves the
defaults in the Tesseract C\texttt{++} API and assumes the input is a single
block of text (\code{PSM\_SINGLE\_BLOCK}) by default, though in our
experience OCR can be significantly improved in some cases by changing
the Page Segmentation Mode. We have found for most problems
\code{PSM\_AUTO}, which attempts to detect the correct page
segmentation automatically, is a good default, though any of the
currently \Sexpr{length(grep("^PSM_",ls('package:Rtesseract')))}
available Page Segmentation modes might be best for different
applications.
%% Curiously,
%% \code{PSM\_AUTO} is the default for the tesseract command line tool
%% but not for the API.
The current Page Segmentation Mode can be found by calling
\code{GetPageSegMode()} on a \code{TesseractBaseAPI} object,

<<>>=
GetPageSegMode(api)
@

Page Segmentation Mode can be set in \pkg{Rtesseract} in multiple
ways: in the call to \code{tesseract()},

<<eval = FALSE>>=
api = tesseract(smithburn, pageSegMode = "PSM_AUTO")
@

in the \code{...}  arguments for any of the high-level functions,  %% @MATT - do we define high-level clearly early on?
e.g.,

<<>>=
GetText(smithburn, pageSegMode = PSM_AUTO)
@

or by using \code{SetPageSegMode()} on an existing
\code{TesseractBaseAPI} object,

<<>>=
GetPageSegMode(api)
SetPageSegMode(api, mode = PSM_AUTO)
GetPageSegMode(api)
@

The available Page Segmentation Modes are listed and enumerated values
in the \pkg{Rtesseract} package, and can be found with,

<<>>=
Rtesseract:::PageSegMode
@ 

The second of these ``meta-variables'' is the Tesseract Engine Mode, which
determines which OCR engine is used to recognize text.
Tesseract operates by recognizing elements of individual characters, and different OCR engines recognize these elements using different algorithms.
Tesseract ships with additional OCR algorithms which can be used individually or in
combination with Tesseract's original line-finding algorithm.
For Tesseract versions 3.05+, an additional OCR engine ``Cube'' is included, and for Tesseract versions 4.00+ a Long Short-term Memory (LSTM) neural network-based OCR engine is included.
Although the use of the other OCR engines in combination with Tesseract can potentially
increase accuracy, their use can add significant computational load
(between 6--10x) and require appropriately trained language files, so
they are not used by default by \pkg{Rtesseract}'s OCR functions. For
a comparison of the accuracy and performance of each of these options,
please refer to
\url{https://github.com/tesseract-ocr/tesseract/wiki/4.0-Accuracy-and-Performance}.

The current Engine mode can be found by calling \code{oem()} on a
\code{TesseractBaseAPI} object,

<<>>=
oem(api)
@ 

Similar to Page Segmentation Mode, the Engine Mode can be set in
several ways. However, unlike Page Segmentation Mode, the Engine Mode
needs to be set at initialization, which means you cannot set it on an
already initialized \code{TesseractBaseAPI} object. %Duncan, this correct?

The Engine Mode can be set either in the call to \code{tesseract()}

<<>>=
api = tesseract(smithburn, engineMode = "OEM_TESSERACT_ONLY") 
@

or in the call to a base level function,

<<>>=
GetText(smithburn, engineMode = "OEM_TESSERACT_ONLY")
@ 

Notice that \code{GetText()} is being called on the file
name. Attempting to set the engine mode on an already created
\code{TesseractBaseAPI} object will have no effect, e.g.,
\code{GetText(api, engineMode = "OEM\_CUBE")}.

\subsection{Image Manipulation and Pre-processing}

\subsubsection{Rotating Text}

<<child = "rotateZoom.Rnw">>=

@ 

\subsubsection{Pre-processing the Image}

Consider the image in Fig. \ref{fig:base}. When we perform OCR on this, Tesseract
treats the highlighted text in "yellow" as a dark rectangular block
and ignores it.

For example, some of the words in the image present in the highlighted text are ``Drs. Dick and
Haddow.''  Are these in the words returned by Tesseract?

<<>>=
t0 = tesseract(smithburn)
w0 = GetText(t0, level = "textline")
grep("Drs. Dick and Haddow", w0, value = TRUE)
@ 

It seems not. In this case, the text under the highlighted area is
being lost. We need to pre-process the image to remove the highlight
but not the text before we pass it to Tesseract to have the
possibility of recovering this text.  There are many options for
pre-processing images, including Imagemagick, OpenCV, Leptonica, or
even \proglang{R} itself operating on the image as a matrix or array. Leptonica
is already used by Tesseract internally to do some pre-processing of
the image, e.g., de-skew, remove lines, etc. Tesseract conducts OCR
off a \code{Pix} object created by Leptonica after this
pre-processing. It would be computational efficient if we could
pre-process the image as a \code{Pix} object in memory to be handed
directly to Tesseract rather than write the image to an intermediate
file on disk only to be read and converted again to a \code{Pix}
object. For this reason, we include some bindings to the Leptonica
library in the \pkg{Rtesseract} package for basic image processing.

To remove the highlight using Leptonica from within \proglang{R}, we
need to convert the image from color to gray-scale or 8 bits per
pixel, and thereby convert the yellow area to a light gray.  We do
this with the \code{pixConvertTo8()} function,

<<>>=
pix = pixRead(smithburn)
pix1 = pixConvertTo8(pix)
@

We can test if this is sufficient to recover the text under the yellow
highlighted region by calling \code{tesseract()} directly on the
modified image:

<<>>=
t1 = tesseract(pix1)
@

Then we call, e.g., \code{GetText()} as before, e.g.,

<<>>=
w1 = GetText(t1, level = "textline")
@

and check whether the words we see under the highlighted text are in
the words returned by tesseract for this modified image:

<<>>=
grep("Drs. Dick and Haddow", w1, value = TRUE)
@ 

Indeed, they are there.

\subsection{Using Leptonica to identify lines}

A primary motivation underlying the\pkg{Rtesseract} package is that we need context from the rendered page to make sense of the results of the OCR.
Rather than just getting the text back, generally, we want information
about the location and size for each character, word or line
identified by tesseract so that we can use it to intelligently
interpret the content.  This includes identifying document and section titles in the document based on the size of the text.  In this example, we'll look at the locations of lines on the page in order to be able to interpret data within tables.  Again, consider the previous image in Fig. \ref{fig:base}.

We can clearly see in the image that the data in TABLE 1 are arranged by row and column.
The column headers are separated by two horizontal lines spanning the
width of the text on the page.  The final 4 columns have two column
headers that span all 4 columns.  The header in column 4
(``NO. STRAINS ISOLATED'') is on 3 separate lines but clearly
differentiated from the other data rows in the table due to the
horizontal line separating the header from the data rows.

Firstly, the \code{tesseract()} function allows us to get the
horizontal and vertical locations of the words on the page.  So we
could use these to attempt to recover the tabular nature of this data.
In this particular example, we can do this effectively and relatively
simply.  However, in images with tables that have center-aligned
columns (e.g. the ``Year Isolated'' column) or with many empty cells, this is more
complicated and can lead to ambiguity regarding which column a cell
belongs.  Instead, we can separate the values into their respective
columns much more effectively if we can identify the vertical lines
between columns.  Similarly, we can separate the column header cells
from the data cells below if we know the locations of the horizontal
line(s) that divide them.  Furthermore, we can determine the end of
the table and the continuation of the regular text below it if we can
find the final horizontal line in this example.

Given the motivation of finding the vertical and horizontal lines on
the page, let's explore how we can do this with \pkg{Rtesseract}.
Unfortunately, we have to do this outside or separately from the OCR
stage of recognizing the text.  The OCR process Tesseract performs
actually identifies and removes the lines from the image before it
recognizes the remaining text on the image.  This aids the accuracy of
the OCR.  However, it does not allow us to query the locations of the
lines it removed.  Accordingly, we must replicate approximately the
same computations it performs to identify the lines and their
locations.  Fortunately, the basic ideas of this process are described at
\url{http://www.leptonica.com/line-removal.html}.  The \pkg{Rtesseract}
package provides corresponding (and higher-level convenience)
functions to effect the same computations.

Our aim here is to simply identify the locations of the horizontal and
vertical lines in the image before or after we perform the OCR.
Note that these steps are entirely independent of the OCR.
When we pass the original/raw image to OCR without modifying
it,
Tesseract will do many of the same computations we do, but for the purposes of removing the lines to increase OCR accuracy. %% Does not make sense

We start by reading the image:

<<>>=
p1 = pixRead(filename = smithburn)
@

We immediately convert the image to gray-scale with \code{pixConvertTo8()}:

<<>>=
p1 = pixConvertTo8(pix = p1)
@

(Note that this creates a new \code{Pix} and allows the previous one to be
garbage collected as we assign the new image to the same \proglang{R} variable.)

The next step is to correct any skew or orientation in the image.
This can be important in detecting horizontal and vertical lines that
may be at an angle because the scan was done with a slight rotation.
There is a function \code{deskew()} to do this.  However, we will show
the steps as they illustrate some of the leptonica functions available
in the\pkg{Rtesseract} package.

To detect the skew, we must first create a separate binary image using
\code{pixThresholdToBinary()}.  Then we call \code{pixFindSkew()} to
determine the actual angle of the skew.  Then we rotate the original
image by that skew angle:

<<>>=
bin = pixThresholdToBinary(pix = p1, threshold = 150)
angle = pixFindSkew(pix = bin)
p2 = pixRotateAMGray(pix = p1, angle = angle[1]*pi/180,
                     grayVal = 255)
@

When the page is deskewed, areas in the corners of the page have to be filled to maintain a rectangular image.
The 255 in the call to \code{pixRotateAMGray()} corresponds to white and
means that any pixels that are ``revealed'' by rotating the ``page'' are
colored white.
We can specify any
gray-scale color value we want here between 0 (black) and 255 (white).

\begin{comment}
It is useful to look at the resulting image:

<<fig = TRUE>>=
plot(p2)
@
\end{comment}

With the reoriented image, we call the\pkg{Rtesseract} function
\code{getLines()} to get the coordinates of the lines in the image.
We'll first extract the horizontal lines.  For this, we pass the image
and then describe a ``mask'' or box that helps to identify contiguous
regions in the image corresponding to the lines we want.
For
horizontal lines, we want wide and vertically short boxes.  We
describe this box with the width and height of the box, e.g.,
This mask is important for identifying lines that might be slightly broken due to poor quality scans of light or thin lines on the page.
Broken lines with white space between each other less than the width (horizontal) or height (vertical) of the rectangle are considered to be connected. 
Hence when the rectangle is wide (horizontal) or tall (vertical), lines are combined more agressively.

<<>>=
hlines = getLines(pix = p2, hor = 51, vert = 3)
hlines
@

This returns a list with each element identifying the (x0, y0) and
(x1, y1) coordinates describing a single line.

We can add these lines to the display of the image to verify they
appear where we expect:

\begin{figure}
<<fig = TRUE>>=
plot(p2)
lapply(hlines,
       function(tmp) {
           lines(tmp[c(1, 3)], nrow(p2) - tmp[c(2, 4)], col = "red",
                 lty = 3, lwd = 2)
       })
@
\caption{}
\label{}
\end{figure}

Note that we had to subtract the y coordinates from the number of rows
of the image as \proglang{R} plots from bottom up and the coordinates for the
lines are relative to row 1 and go down.

\code{getLines()} performs several operations on the image.  It first
uses the \code{findLines()} function to return an image that contains
either the horizontal or vertical lines.  We'll first get the
horizontal lines with

<<<>>=
h = findLines(pix = p2, hor = 51, vert = 3)
@

We specify the \code{Pix} to process and then the horizontal width and
vertical height (in pixels) that define a window or a region.  This
fills in gaps in potential lines within the region.  Leptonica moves
this around and fills in gaps within it if the rest of the pixels
within the region are not white.  \code{findLines()} uses the function
\code{pixCloseGray()} to do this.

Let's look at the resulting image to see what lines it detected:

<<>>=
plot(h, invert = TRUE)
@

Note that the background is black, corresponding to values of 0 in the
Pix.  We see the 4 primary lines that span most of the width of the
image.  The second one down (at y = 3600) has several short gaps.   We
also see many short line segments above the first line and to the
right of the image.   The ones on the right correspond to parts of the
letters in the rotated text in the image indicating ``Downloaded from
http://www. ...''.  If we made the vertical height of our mask larger,
we may discard these but at the risk of including other elements.


We are looking for rows in the image that have many black pixels (with
value 1).  Currently, \code{findLines()} returns an image (Pix
object).  By default, this shows the potential lines and removes
everything else.  (We can also return the image with the lines removed
and the text remaining.)  We convert this to a matrix of 0s and 1s in
R:

<<>>=
m = pixGetPixels(h)
@

(or simply \code{h[,]})

\begin{comment}
We find the rows of this matrix that have 1000 or more
black pixels.  We do this with

<<>>=
w = rowSums(m) > 1000
@

The value 1000 is context-specific.  It includes lines that span 1000
pixels or more, and also lines that have several smaller segments
whose length adds up to 1000 pixels.  1000 is about 25\% of the
columns.  This might be too large to detect the shorter line segments
on the right of the table header, but it is probably enough to ignore
the small line segments that don't interest us.

How many rows of the matrix satisfy this criterion:

<<>>=
table(w)
@

So we have 46 which is a lot more than the 6 we see in the original
image.

We can see these horizontal lines on the image with

\begin{figure}[h]
<<figure = TRUE>>=
plot(h, invert = TRUE)
abline(h = nrow(m) - which(w), col = "red")
@
\caption{}
\label{}
\end{figure}

Note again that the rows in the matrix go from 1 to nrow, while on the
plot, they go down the page.  This is why we get the height with
\code{nrow(m) - which(w)}.

We can see we get the 4 lines that span the width of the text area,
but not the two shorter line segments.
\end{comment}

We are looking for lines which span more than a certain
threshold.  This threshold will be context specific, but we can find a
suitable one via some experimentation.  An initial guess of 1000 pixels
resulted in discarding the two shorter segments.  So let's relax the
threshold of requiring 1000 black pixels.  We'll use 10\% of the
columns:

\begin{figure}[h]
<<>>=
w = rowSums(m) > 0.1 * ncol(m)
plot(h, invert = TRUE)
abline(h = nrow(m) - which(w), col = "red")
@
\caption{}
\label{}
\end{figure}

So we detect these two shorter lines and no other additional lines
(except the original 4 longer lines).  However, now we have 74 rows in
the matrix that satisfy this criterion.  Yet we see only 6 lines on
the plot.

Let's look at the row numbers for these 74 rows:

<<>>=
which(w)
@

It may not be clear, but these are grouped together.  Each of the red
lines we see on the plot is actually a collection of multiple adjacent
lines.  This is clearer if we compute the difference between the row
numbers:

<<>>=
diff(which(w))
@

We see a collection of 1s meaning that these are adjacent rows, then a
large jump between row numbers, followed by a collection of 1s and on.
So it is clear these are grouped.  We want to combine adjacent lines
into a single group and have a group for each conceptually separate
line on the image.  One way to do this is run length encoding (RLE):

<<>>=
rle(diff(which(w)))
@

Another way is very similar but more explicit and gives us a little
more control.  We find where the differences between successive row
numbers is greater than 2 and then we run a cumulative sum which gives
us group labels:

<<>>=
g = cumsum( diff(which(w)) > 2 )
@

We can use g to split the rows into groups

<<>>=
rowNums = which(w)
ll = tapply(seq(along = rowNums), c(0, g),
            function(i) m[rowNums[i], ])
names(ll) = tapply(seq(along = rowNums), c(0, g),
                   function(i) as.integer(mean(rowNums[i])))
@

(Note that we added 0 to the start of the vector g since g has one
less element than w since we were computing pairwise differences.)
The computation is a little awkward.  We split the indices of rowNums
and then have to use those to index back into rowNums to find out
which rows of m we need to extract for this group.

The third line puts the average of the row numbers of each group as
the name of that element.  This will be convenient for mapping this
back to the general vertical area.

Each element of \code{ll} is a matrix.  For each of these, we want to
combine them to identify the start and end of a line.  Each may have
several separate line segments.

Let's consider the first element of \code{ll}.  It has 14 rows and 4050
columns.  It is useful to query how many 0s and 1s there are?

<<>>=
table(ll[[1]])
@

These are about equal: 27000 0s and 30000 1s.  (In this setup, 1 is
black.)  So there are many white pixels even though the black pixels
appear as solid line on our plot.  But that was because we just drew a
straight line at these vertical positions, regardless how many pixels
were white.  Of course, we selected these rows because they had many
black pixels.

Since we think these rows make up a single line, let's aggregate across the adjacent
rows to form a single aggregate row using \code{colSums()}:

<<>>=
rr = colSums(ll[[1]])
@

If this is a solid line across most of the page, we'd expect most
pixels to be filled, perhaps with a few short gaps.  Each element of
rr contains the number of rows in this group that had a black pixel.
This might be 1 or 14, with only one row or all of them having a black
pixel.  We might look at these counts or just consider whether any row
has a black pixel in this column.  We are looking for a rule to mark a
pixel in the aggregated line as black or white.

If we look at the vector \code{rr}, we see a lot of 0s at the
beginning and the end.  These are the margins where the line is not
present.

If we want to threshold this aggregate line to have a black pixel in a
column if \textit{any} of the pixels in the adjacent rows defining it
has a black pixel, we can do this with

<<>>=
rle(rr > 0)
@

We get the two margins and a long run of 2895 pixels.  If we require
at least half of the 14 rows to have a black pixel in a column, we get
the same groups:

<<>>=
rle(rr >= 7)
@

Let's consider one of the shorter lines.  These are elements 2 and 3
of \code{ll}.  Again, we compute the column sums for the adjacent
lines in this group:

<<>>=
rr = colSums(ll[[2]])
table(rr)
@

Most are 0.  There are 9 adjacent rows in this group.  We see no
columns that have at least one black pixel but less than half the
number of rows.  Let's draw points at each of the columns that have a
black pixel in at least half the rows:

\begin{figure}[h]
<<>>=
z = which(rr >= 4.5)
plot(h, invert = TRUE)
points(z, nrow(m) - rep(as.integer(names(ll)[2]), length(z)),
       col = "green", pch = 16, cex = 0.5)
@
\caption{}
\label{}
\end{figure}

This appears to capture the short horizontal line spanning the four
columns on the right of the table.  So this appears to be a reasonable
approach, at least for this image.

\subsubsection{Vertical Lines}

We can follow the same process to identify the locations of the
vertical lines.  We've already deskewed the original image, so we don't
have to do that.  We can call \code{findLines()} and this time specify
a window that is tall and narrow to capture the vertical parts.  We
should experiment with these values.  One thing to note is that
characters have vertical components more than horizontal components,
e.g. 1, L, M.  If we make the height of the window too small, we'll
pick up more false positives.

<<>>=
v = findLines(p2, 3, 101)
@

Again, we can plot this and see where the lines are.

\subsubsection{Removing the Lines}

We don't have to remove the lines before we pass the image to
tesseract as tesseract will remove them for us.  However, it is
interesting to see how well we can do.  Instead of finding the
locations of the lines (with \code{getLines()}), we'll remove them
from the image.  This uses the same basic approach and sequence of
steps.  However, rather than extract the line coordinates, we use
\code{pixAddGray()} to overlay the images with the horizontal and then
the vertical lines onto the original image.  This removes the lines
from the original image.  We do this directly from the image file with

<<>>=
p1 = pixRead(smithburn)
p2 = pixConvertTo8(p1)
p2 = deskew(p2)
p3 = findLines(p2, 51, 1, FALSE)
p4 = findLines(p2, 3, 101, FALSE)
p = pixAddGray(p2, p3)
p = pixAddGray(p, p4)
plot(p)
@

Let's compare this to the original image.

\begin{figure}[H]
<<>>=
plot(p2)
@
\caption{}
\label{}
\end{figure}

The horizontal and vertical lines are essentially gone.  We can
certainly see light grey ``smudges'' in some places where the vertical
lines were.  We also see some short horizontal line segments
remaining.

We can experiment with the horizontal and vertical values we pass to
\code{findLines()}.  Increasing the horizontal width from 51 to 75
cleans up the horizontal lines.

We can also change the thresholds for creating the binary images.  We
can also threshold the final image (p) at this point.  We set any
value 50 or above to 255, i.e., white, with

<<>>=
pp = pixThresholdToValue(p, 50, 255)
@

and when we plot the resulting image

\begin{figure}[H]
<<>>=
plot(pp)
@
\caption{}
\label{}
\end{figure}

The gray marks on the vertical lines have disappeared.   Note however
that it removed some of the text, specifically the rotated text on the
right and the URL within that text that is colored blue in the
original image.   Fortunately, we don't care about this text for our
application.

We can compare the image we created with the lines removed to the one
tesseract creates.   We do this with

<<>>=
tesseract(smithburn, pageSegMode = "psm_auto",
          textord_tabfind_show_vlines = 1)
@

This creates a PDF file named vhlinefinding.pdf.

\begin{comment}
  
  \subsection{Augmenting the dictionaries}

There are a plethora of variables that can be adjusted, so for space
considerations we will highlight the ones we have found the most
useful in our work.

%% Might need to cut this - cannot get it to work correctly
To continue the example above, on inspection of
the results of the OCR there are some domain-specific terms that are
not being recognized correctly.  While it might be possible to correct
these in post-processing, it is preferable to tweak the settings of
the OCR engine to allow the correct recognition in the processing
stage.  The above document includes the two terms that are consistently
mis-recognized; {\it{``Anopheles''}} (a scientific name) and ``Zika''
(a virus name).  \code{Tesseract} allows
user-specified words to be added to the dictionary of correct matches
through a ``user-words'' file.  \pkg{Rtesseract} allows you to specify
this file, which will then be used by the OCR engine to resolve
ambiguities:

<<>>=
writeLines(c("2 z' 1 1 "), "eng.unicharambigs")
writeLines(c("Zika", "Anopheles"), "eng.user-words")
@

For comparison, we will create a second \code{TesseractBaseAPI} object
with new settings,

<<>>=
api2 = tesseract(smithburn, pageSegMode="psm_auto",
                 opts = list(user_words_file = file.path(getwd(),"eng.user-words"),
                             user_words_suffix = "user-words",
                             language_model_penalty_non_dict_word = 0.75,
                             # tessedit_char_blacklist = "a",
                             tessedit_enable_dict_correction = "1"))
@

And then we can check if the results are improved,

<<>>=
grep("^An", GetText(api), value = TRUE)
grep("^An", GetText(api2), value = TRUE)
@ 

Extending this idea, we can explore which characters have the lowest
confidence and/or occurrence in the text.

<<>>=
symbolBB = GetBoxes(api, level = "symbol")
symbolConf = sort(tapply(symbolBB[,"confidence"], row.names(symbolBB), median))
# Lowest eight 
symbolConf[1:8]
@

As before, visually inspecting these low-confidence characters using the \code{plotSubImage} function for a single bounding box, or the \code{plotSubsets} function to create a panel
plot of multiple bounding boxes can help verify that
they are correct or diagnose issues (e.g., Fig. \ref{fig:subsets}).

\begin{figure}[H]
<<fig = TRUE>>=
idx = which(symbolBB[,"confidence"] < 70)

plotSubsets(symbolBB[idx,], img = png::readPNG(f))
@
\caption{An example of the \code{plotSubsets} function, which creates panel plots of
  individual subsets of the image along with the associated OCR result
  and confidence.  In this example, the ``ã'' character is oft
  mis-recognized.}
\label{fig:subsets}
\end{figure}

Say we are primarily interested in the
abstract.  Using the bounding boxes or other coordinates as guides, we
are able to adjust the OCR-ed area without leaving \proglang{R}.  Typically, to do
this would involve partitioning the image into subsets using an image
editing tool (e.g., Gimp, Darktable, Photoshop), and then running the
OCR on each of these subset images.  This process involves leaving R
and involves steps that may be difficult to replicate.  However,
\pkg{Rtesseract} allows the user to specify a rectangle of interest
using the \code{SetRectangle} function.  This function takes a
\code{TesseractBaseAPI} object and the left, top, width and height of
the sub-area as additional arguments (the dimensions of the full input
image can be found with \code{GetImageDims}).  Once set, the next call to the
\code{TesseractBaseAPI} will only recognize text within this rectangle
- no external image manipulation required.  Since we know that the
abstract is between the ``Abstract'' header and the ``Keywords''
tagline, we can easily find the coordinates of these using the
bounding boxes and then only recognize text within this area:

<<>>=
bb = GetBoxes(api)
idx = grep("Abstract|Keywords", rownames(bb))
bb[idx,"top"]
GetImageDims(api)

SetRectangle(api, 0, 950, 4000, 1800 - 950)
GetText(api, level = "textline")

#Reset rectangle
GetImageDims(api)

SetRectangle(api, 0, 0, 4013, 3210)
@ 


\end{comment}

\section{Querying and setting variables}

Many additional customizable variables allow further fine control of the OCR engine
and are accessible through \proglang{R} via the \pkg{Rtesseract}
package.
These include white- and black-list
characters, custom user-added patterns and words, etc.
To find the current settings for the tesseract OCR engine we use the
function \code{PrintVariables()},

<<>>=
v = PrintVariables(api)
v
@ 

At the time of writing, this returns \Sexpr{length(PrintVariables())}
variables. \code{PrintVariables} returns a named character vector,
with the names corresponding to the variable name and the value
corresponding to the current value. We can search for options,

<<>>=
grep("blacklist", names(v), value = TRUE)
@

and then adjust those options via a call to
\code{SetVariables},

<<>>=
SetVariables(api, tessedit_char_blacklist = c("^", "_", "|"))
@

Alternatively, we can also set variables when the api is created by \code{tesseract()}

<<>>=
api = tesseract(smithburn, opts = list(tessedit_char_blacklist = c("^", "_", "|")))
@

Full documentation of user-adjustable variables
is available at
\url{https://github.com/tesseract-ocr/tesseract/blob/master/doc/tesseract.1.asc}.

\subsection{Specifying the recognized language}

Tesseract requires trained language files in order to operate.  
By default, \pkg{Rtesseract} searches either \texttt{/usr/bin/local}
or \texttt{/usr/share/} directory for
installed trained language files, but when we create the
\code{TesseractBaseAPI} we can specify both a different
language, say Spanish

<<>>=
api = tesseract(smithburn, lang = "spa")
@

and an alternative location for the language data files,

<<eval = FALSE>>=
api = tesseract(smithburn, lang = "spa", datapath = ".")
@

We can find the current location where the API is searching for
language data with \code{GetDatapath()},

<<>>=
GetDatapath(api)
@

Since Tesseract allows us to create
trained data files for custom languages, the ability to specify the
path of the language files alleviates the need to place these language
data files in the default search location.  More information on
training Tesseract and providing language files can be found at
\url{https://github.com/tesseract-ocr/tesseract/wiki/Training-Tesseract}.  

\subsection{Evaluating different configurations}

Sometimes, optimal settings are found through experimentation.  By
creating persistent \code{TesseractBaseAPI} objects via the \code{tesseract()}
function, two or more \code{TesseractBaseAPI}s can be created for the
same image, e.g.

<<>>=
api = tesseract(smithburn, pageSegMode = "PSM_AUTO")
api2 = tesseract(smithburn, pageSegMode = "PSM_SINGLE_BLOCK",
                 opts = list(
                     tessedit_char_whitelist = c(letters,
                                                 LETTERS,
                                                 0:9,
                                                 ".")))
@

Doing this allows us to compare the impact of various settings
to fine tune the OCR for individual cases.  Once a suitable set of
adjustments have been found, we can exported these settings using
\code{PrintVariables()}. These custom variables can then be easily set
using again \code{SetVariables()} or the \code{opts} argument in the
\code{tesseract()} function.

\subsection{Writing results directly to file}

\pkg{Rtesseract} includes functionality to export the results of OCR
to many conventional file types, including BoxText, HOcr/HTML , OSD
(Orientation and Script Detection), TSV (tab separated values), and
PDF.  Of these, \code{toPDF()} is unique in that it creates a searchable
PDF document, not a plain-text document.

\subsection{Metadata}

Lastly, \pkg{Rtesseract} allows users to access metadata about the OCR
engine and the input image.  The current version of Tesseract can be
returned with 

<<>>=
tesseractVersion()
@

while the capabilities for
reading various image formats via leptonica is provided by,

<<>>=
leptonicaImageFormats()
@ 

Information about the source image are also accessible through \proglang{R} using

<<>>=
GetInputName(api)
GetImageInfo(api)
GetImageDims(api)
GetSourceYResolution(api)
@

\section{Package Installation}

In order to use \pkg{Rtesseract}, users must first have both Tesseract
and its dependency, Leptonica, installed.  Tesseract is available from
\url{https://github.com/tesseract-ocr/} and Leptonica from
\url{https://github.com/DanBloomberg/leptonica}.  Pre-compiled
binaries are available for most platforms for the current stable
version (v3.05.01), though \pkg{Rtesseract} also requires libraries to
compile which are not often included those for Windows
platforms. Therefore, if installing pre-built binaries of v3.05.01 for
Windows is desired, we suggest installing Tesseract from the Rwinlib
repository (\url{https://github.com/rwinlib/tesseract}), which also
includes the required libraries for Windows.  As previously mentioned
\pkg{Rtesseract} also works with the development version of Tesseract
(4.00+), which is currently available from
\url{https://github.com/tesseract-ocr/tesseract/releases}. At the time
of writing, we are not aware of pre-built binaries for v4.00 which
include the required libraries as well. However, to build Tesseract
from source requires many additional pieces of software, including
leptonica v1.74+, automake, libtool, pkg-config, aclocal, autoheader,
and autoconf-archive.  Additionally, users will also need to install
the training data for the language(s) used in the documents to be
processed. Currently, there are pre-trained data sets for 140+
languages available from \url{https://github.com/tesseract-ocr}.
Users can also provide their own trained data.  For more information
about training Tesseract, please see
\url{https://github.com/tesseract-ocr/tesseract/wiki/Training-Tesseract}.
\pkg{Rtesseract} includes some functions that assist with training,
including \code{ReadBoxFile()}.  Finally, the \pkg{Rtesseract} package
itself can be installed from source at
\url{https://github.com/duncantl/Rtesseract}.

% Different section title
\section{Future directions}

\pkg{Rtesseract} provides both easy access to basic OCR functionality
for simple use-cases as well as access to low-level control parameters
for more advanced uses.

By allowing recovery of additional
information generated by Tesseract, such as confidence levels and
bounding boxes, \pkg{Rtesseract} supports novel applications of
inputting data from images into \proglang{R} (e.g., spatial analysis of text).
Leveraging free, open-source software allows full integration of the
work-flow of text from images into \proglang{R}.

\bibliography{Rtesseract.bib}
\end{document}
