% !TeX root = RJwrapper.tex


% Things we should add or make certain are there 
%
% + Examples showing setting some of the options and why they matter
%   e.g. language, white list
% + Querying current options/configuration settings.
%
% + Getting the dimension of the image 
%    See  getImage.R 
% + Limiting to a particular  subregion.
%     See ./setRectangle.R and the SetRectangle example.
%
% + Setting an image after processing it in R first. Have to check this works. 
%     SetImage_raw
%

%     NOTES: Too disjoint - leaving too much ot the reader to figure
%     out
%     References too far away from their target
%     More explanations to some of the parameters
%     Going for chatty

%     Code is part of sentence or figure - no ". <CODE>"

%     Leaving too much to the reader to figure out -
%     be more explicit in what we are doing and why we are doing it.
%     Spell out motivation for the various functions - explain the
%     funcitons in text before running off and using them.

%     Connect the text to the code better - Break up the code into
%     small segments 
%     Short - motivation for each section

%     Highlight that give us the means to query and then modify the
%     tool - changes computational model

%     All names mirror the C++ API, so you can find attitional
%     documentation in the Tesseract documentation.

%     Tesseract is rich - this provides easy way to modify,
%     programmatically explore, able to "sit" with and drive
%     More than a single function / atomic action
%     is piece of a larger workflow.


\title{Rtesseract: A package for Optical Character Recognition (OCR)
  from R}

\author{by Matthew B. Espe and Duncan Temple Lang}

\maketitle

\begin{abstract}

We created the \pkg{Rtesseract} package to support working with text data
present in images. \pkg{Rtesseract} is an R package that provides
access to Tesseract, an Open Source C++ library for Optical Character
Recognition (OCR), from within R.  At its simplest, \pkg{Rtesseract}
allows an R user to recover of text from an image as lines, words or
individual characters.  We can also obtain the location of these text
elements, possible alternatives and the predictive ``confidence'' for
each.  These are often necessary for interpreting the text, e.g., for
multi-column layout, or tables.  The package provides access to much of
the \code{TesseractBaseAPI}, allowing full customization of OCR
behavior and also exporting the results in various formats (e.g. PDF)
and also visualizing them within R itself. With the \pkg{Rtesseract}
package, users are able to create a fully programmatic workflow to
extract text data into R for subsequent analysis.

\end{abstract}

\section{Introduction}\label{intro}

<<setup, echo=FALSE, message=FALSE, warning=FALSE>>=
library(Rtesseract)
library(knitr)

# Globally set output to be limited to 4 lines
knit_theme$set("print")
opts_chunk$set(out.lines = 4)
opts_chunk$set(highlight = FALSE, background = "white", prompt = TRUE,
               comment = '')


# # the default output hook
# # From https://github.com/yihui/knitr-examples
hook_output = knit_hooks$get("output")
knit_hooks$set(output = function(x, options) {
    if (!is.null(n <- options$out.lines)) {
        x = unlist(stringr::str_split(x, "\n"))
        if (length(x) > n) {
            # truncate the output
            x = c(head(x, n), "....\n")
        }
        x = paste(x, collapse = "\n")  # paste first n lines together
    }
    hook_output(x, options)
})

          
@ 

There has been an enormous increase in the amount of text data that we
work with from social media, reports, scholarly articles, job
applications, etc. These often come as plain text or structured documents
such as word-processing files, HTML, PDF. However, there are many
documents of interest that are only available as scanned or
photographic images (e.g., old books, newspapers).
%we want tables of data or text for NLP, lists of entries
Optical Character
Recognition (OCR) is a technique to infer characters, words or lines
of text from an image, thereby making it accessible for data
analysis. There are many OCR software tools available (e.g., Adobe
Acrobat, PDFPen Pro, ABBY), though these typically only output text results
to a file on disk.

Although OCR technology has steadily improved over the years, OCR
results are rarely perfect. We may not care about these errors if they
are inconsequential to subsequent analysis. However, often these error
do impact our analysis and we need to correct them before
preceding. Therefore, OCR is commonly part of a workflow that involves
pre- and post-processing steps to verify and increase the accuracy of
the text extraction. For example, an image might manipulated (e.g.,
cropped, increased contrast, decreased noise, etc.) using image
editing software to make the text ``stand out'' prior to OCR. Then
after OCR has been run we will check if the results make sense. In
some cases there are obvious issues, such as nonsense words or garbled
text, but in other cases we might want to focus our attention on the
areas where the OCR engine had the least confidence. When we have
prior contextual knowledge about the correct matches, such as only
numbers or certain characters or words occur in the text, we can
leverage this information to correct errors in the post-processing
steps. When the text is arranged in columns or blocks, we will want to
rearrange the text into the correct order. Finally, we will analyze
the text results using methods such as NLP. Using OCR software which
only returns text results, our full OCR workflow would involve 1)
multiple platforms to perform pre-processing, OCR, post-processing,
and analysis, 2) using pre- and post-processing steps to overcome
limited control over the OCR process itself, 3) limited options to
programmatically control and reproduce each step in the workflow, and
4) no access to OCR intermediate products, such as confidences,
alternative matches, location on the page, etc., which are helpful for
assessing the certainty in the each returned result.

In our work extracting and analyzing text data present in images from
R, we desired a tool which would overcome the limitations above. In our
search for solutions, we found Tesseract, a long-standing open source
OCR library originally created in 1985 and currently under active
development. Accessing the Tesseract API from R provides us with the
ability to bring pre-processing, OCR, post-processing, and analysis
under complete programmatic control within R. The \pkg{Rtesseract}
package provides 1) a simple high-level function to extract the text
from one or more images, 2) the ability to also obtain important
auxiliary data (e.g., the confidence in each prediction, the location
within the image), and 3) customization of how the OCR is done.

% OCR is improving, but it is not perfect. What is pipeline - concrete examples:
% Why pre-process?
% Why post-process?
%
% 1. We picked tesseract
% 2. so did ...
% 3. why ours is better for workflow

% Use the term certainty/uncertainty

\begin{comment}
In response to our needs, we developed the \pkg{Rtesseract} package,
an interface to the open source OCR library Tesseract. Tesseract was
created 1985, was released for open source in 2005, and has since been
actively maintained and developed by the open source community
(including developers at Google). Recent innovations in Tesseract
include the integration of LSTM Deep Learning networks (versions >=
4.0) for increased speed and accuracy. By interfacing with Tesseract,
the \pkg{Rtesseract} package provides us 1) a simple high-level
function to extract the text from one or more images, 2) the ability
to also obtain important auxiliary data (e.g., the confidence in each
prediction, the location within the image), and 3) customization of
how the OCR is done. Additionally, the results are accessible directly
within R as text and/or a data frame of locations and confidences.
\end{comment}

This approach using \pkg{Rtesseract} tightly integrates pre- and
post-processing steps with the actual OCR. For example, we can focus
on a sub-region of the image, or enhance the image with some computer
vision techniques before performing the character recognition. We can
specify a domain-specific vocabulary, or identify unconventional
fonts. We can adjust the OCR behavior via configuration variables to
correct errors in the recognition step rather than relying on
post-processing. This tight integration simplifies workflows that
otherwise rely on invoking stand-alone applications typically meant
for human interactive use. Furthermore, metadata about the resulting
text can aid and inform subsequent analyses. For example, the layout
of the text into blocks, the presence of elements such as lines or
other dividers, or even the relative size of the text can all be used
to make sense of the resulting text. Diagnostic pieces of information
such as the confidence in the matches, alternative matches, etc., can
be essential assessing certainty and improving OCR accuracy. If the
OCR tool only returns the text results, as is the case with many OCR
tools, OCR's usefulness is limited to cases where the problem is
either simple or well-covered by the default configuration.

We have not been alone in recognizing the utility of Tesseract. Since
we initially created \pkg{Rtesseract} in April 2015, two other R
packages interfacing to Tesseract have been released. The \pkg{ocR}
package~\citep{Hagmann2014} provides an interface to calling the
Tesseract command line executable. Using the \pkg{ocR} package, the
user specifies the image file and command-line options, Tesseract is
called and writes the results to a file which is then read back into
R. The \pkg{tesseract} package~\citep{ooms2016} takes a similar
approach to \pkg{Rtesseract} package by interfacing to the Tesseract
C\texttt{++} library. However, by design, it provides only
functionality to get the text results from the OCR
process. \pkg{Rtesseract}, from its initial implementation, provides
access to most of the functionality in the Tesseract API and
provides fine-grain control from R to the different steps in the OCR
process. Additionally, \pkg{Rtesseract} provides primative tools which
allow users to easily extend functionality. \pkg{Rtesseract} supports
both the current release (v3.05.01) and development versions
(v4.00.alpha) of Tesseract.

\section{Optical Character Recognition using Rtesseract}\label{desc}

For many simple problems, merely extracting text from an image in a
reliable manner is sufficient. \pkg{Rtesseract} provides simple
interfaces that allow users to provide the path to an image in a
supported format (tiff, JPEG, and png on most systems), and OCR will be
run on the image using the default settings. For example, say we had
an image from an older scientific publication from which we wanted
to extract information (Fig. \ref{fig:base}). We will use this
image to highlight the functionality and common use-cases of the
\pkg{Rtesseract} package. % AWK

% Fix this
<<echo = FALSE>>=
f = system.file("images", "SMITHBURN_1952_p3.png", package = "Rtesseract")
@ 

\begin{figure}[H]
\includegraphics[width=\textwidth]{\Sexpr{f}}
\caption{An example image of a scientific paper which contains text we
  would like to analyze.}
\label{fig:base}
\end{figure}

\subsection{Base Functions}

For the simplest task of retrieving the text contents of the image, we
use the high-level function \code{GetText}.  We pass it the path to a
file containing the image.  By default, it returns the words it finds
in the image in the order they appear,

<<>>=
f = system.file("images", "SMITHBURN_1952_p3.png", package = "Rtesseract")
GetText(f)
@

We sometimes want the results returned as individual characters (symbols),

<<>>=
GetText(f, level = "symbol")
@ 

or sometimes as lines (textlines),

<<>>=
GetText(f, level = "textline")
@

All of which are easy to achieve with the \code{levels} argument in
\code{GetText}. Thereby, the functionality of most basic OCR
applications is provided by the \code{GetText} function. 

\pkg{Rtesseract} includes these three other high-level functions
(\code{GetBoxes}, \code{GetConfidences}, and \code{GetAlternatives})
to extend the information accessible by the user. Similar to
\code{GetText}, these functions can take the path of the image file
and the optional \code{level} argument (except \code{GetAlternatives}
which only operates at the ``symbol'' level).

This practice of calling \code{GetText} followed by another to, e.g.,
\code{GetBoxes} for the same file is a common occurrence when working
interactively with \pkg{Rtesseract}. Yet, it has a distinct
inefficiency - we repeat the OCR recognition the same file merely to
record a different product from the OCR engine. Since OCR is
reasonably computationally intensive, we would like to perform the OCR
once and then query each aspect we want (i.e., the text, the
confidences, the alternatives, \ldots) from this single OCR operation,

Creating a \code{TesseractBaseAPI} object once which can be repeatedly
queried is the preferred method when multiple pieces of information are
needed from the same file. The \code{tesseract} function creates an
instance of the OCR engine (an object of class \code{TesseractBaseAPI}
in R),

<<>>=
api = tesseract(f)
@

We specify the path to the file and any other options controlling how
the OCR will be performed (explained in more detail below in ``Advanced
Use''). We then pass this object to each of the four high level functions
rather than the file name, e.g., \code{GetText(api)},
\code{GetBoxes(api)}, \code{GetConfidences(api)}. By default, the call
to \code{tesseract} doesn't perform the OCR immediately. Instead, it
is only done when we request any results, e.g., calling any of the
functions above, or explicitly calling the \code{Recognize} function
with the \code{TesseractBaseAPI} object as an argument.

To show how merely retrieving text can be of limited usefulness, we
continue the example above. After extracting the text from the
scientific publication, we would like to check the results. However,
checking word by word manually is both tedious and
error-prone. Rather, we might want to focus our attention initially at
those words with the lowest confidences:

<<>>=
confs = GetConfidences(api)
sort(confs)[1:10]
@ 

Clearly, some of these do not look correct. To further investigate, we
might want to see where these low-confidence words occur on the page
by extracting the bounding boxes,

<<>>=
bbox = GetBoxes(api)
@

\code{GetBoxes} extends the information returned by
\code{GetConfidences} by including the left, top, right, and bottom of
the bounding box around the word, symbol, textline, etc. Using these,
we can sort by the confidence,

<<>>=
bbox[order(bbox[,"confidence"])[1:10],]
@

On first glance, it appears the low confidence words might be grouped together in
similar areas on the page. Since we have access to the confidences and
bounding boxes in R, we can treat these just like any other data. For
example, we can group the words into bins by their location left to
right,

<<>>=
lrGroups = cut(bbox[,"left"], breaks = seq(min(bbox[,"left"]),
                                           max(bbox[,"left"]),
                                           length.out = 11))
               
@ 

And then find the median confidence inside each group,

<<out.lines=8>>=
tapply(bbox[,"confidence"], lrGroups, median)
@

We can see there are issues with the right side of the page. Looking
at the image (Fig. \ref{fig:base}), we can see there is rotated text
in the margin that identifies when and where the PDF was
downloaded. This text is not being recognized well by the OCR, but is
also not part of the journal article we are interested in. Because of
this text crosses several textlines, post-processing without location
information would involve searching for ``junk'' at the end of each
text lines. However, we can easily exclude this text in
post-processing with location information in hand. Additionally, while
we could crop this image using editing software outside of R, we will
show later how this area can be excluded from the OCR recognition step
using \pkg{Rtesseract} without leaving R.

To continue, suppose we also wanted to extract the author's
name. Using some prior knowledge that this information is present at
the top of each page, we can focus on the first text line,

<<>>=
GetText(api, level = "textline")[1]
@ 

We know that the lower case letter in ``c.'' is a mistake, but we might
not be sure what the correct letter should be. Helpfully, we can
inspect the alternative matches,

<<>>=
GetAlternatives(api)[6]
@ 

which reveals that ``C'' was a close second in this case.

This example illustrates both the workflow of using OCR to extract
text and highlights why having access to information beyond just the text is so
crucial to a full OCR workflow.

\begin{comment}
<<echo = FALSE>>=
f2 = system.file("images", "IMG_1234.png", package = "Rtesseract")
@ 

\begin{figure}[h]
\includegraphics[width=\textwidth]{\Sexpr{f2}}
\caption{An example image of old data that we would like to recover
  and analyze in R.}
\label{fig:old_data}
\end{figure}

Rather than transcribing these data by hand, we would like to
programmatically extract them into R using \pkg{Rtesseract}. However,
the accuracy is not great:

<<echo = FALSE>>=
api2 = tesseract(f2)

GetText(api2)
@

An immediate question rises: Is the image quality affecting the
results of the OCR. From inspecting the image
(Fig. \ref{fig:old_data}), the text on the right side of the page
appears to be darker than that on the left side. Since we can get the
locations and associated confidences for each individual symbol
recognized by Tesseract, we can handle these just as any other data
problem. We can extract the bounding box information at the symbol
level,

<<>>=
bb = GetBoxes(api2, level = "symbol")
@

group the symbols by location on the page, left to right,

<<>>=
left2right = cut(bb[,"left"], breaks = seq(0, max(bb[,"left"]), length.out = 7))
@ 

and finally inspect the relationship between confidence and location
left to right numerically,

<<>>=
tapply(bb[,"confidence"], left2right, mean)
@

or visually,

\begin{figure}[H]
<<fig=TRUE, echo = FALSE, width = 4, height = 4, out.width = '0.8\\linewidth'>>=
scatter.smooth(bb[,"confidence"] ~ bb[,"left"],
     xlab = "Position of character bounding box on page",
     ylab = "Confidence (higher is more confident)",
     xaxt = "n",
     pch = 16, col = rgb(0,0,0,0.25))
axis(1, at = c(500,2000,3500), labels = c("left", "center", "right"))

@ 
\caption{An example how visualizing the confidences from the OCR
  allows investigation of how location on the page influences
  the results of the OCR engine.}
\label{fig:conf_by_loc}
\end{figure}

It is clear that there is a trend, with the left side of the page
being recognized with less confidence than the right side. We can use
this information to improve the image quality using photo-editing
software or even computer-vision techniques such as OpenCV.
\end{comment}

We found that visualizing the confidences was essential to inspecting
\pkg{Rtesseract} results. Therefore, \pkg{Rtesseract} includes a
plotting method for OCR output. By default calling \code{plot} on a
\code{TesseractBaseAPI} object will create a plot which includes
original image overlaid with the bounding boxes colored (and
optionally filled) by relative confidence (see
Fig. \ref{fig:rtess_plot} for an example). Plotting the
results is especially helpful when adjusting the OCR configuration
settings (discussed below under ``Advanced Use'').

\begin{figure}[H]
<<echo = FALSE>>=
plot(api, fillBoxes = TRUE)
@
\caption{\pkg{Rtesseract}'s plotting function allows the user to
  overlay the original image with the bounding boxes and associated
  confidences.}
\label{fig:rtess_plot}
\end{figure}

\begin{comment}
Similarly, by allowing access to information regarding the bounding
boxes, \pkg{Rtesseract} enables users to use location or size specific
information manipulate the results of the OCR. For example,
identifying the title in the image of the scientific paper from plain
text results would involve using context-based heuristics, but the
task is far simpler with access to information about the text
size. First, we need to extract the bounding boxes at the text-line
level,

<<>>=
bb = GetBoxes(api, level = "textline")
@

And then identify the set of boxes which are the tallest,

<<>>=
rownames(bb)[which.max(bb[,4] - bb[,2])]

@ 

We have used the location of the bounding boxes to identify and
reconstruct text arranged in columns, separating by unrelated elements
(e.g., newspaper content), or even to reconstruct complex tabular
content. In our use, we have found using the bounding boxes preferable
to attempting to simulate the layout of the page by padding the text
with spaces (e.g., the ``layout'' output of many other OCR engines).
\end{comment}

\section{Advanced Use}

Thus far, we have focused on functions in \pkg{Rtesseract} which allow
accessing and manipulating the results of running the OCR engine on an
image. In complex cases, you might want to manipulate the behavior of
the OCR engine itself rather than merely the output. For these
purposes, the \pkg{Rtesseract} packages exposes full access to the
\code{TesseractBaseAPI}, which allows fine-tuning of the OCR engine
for specific cases.

\subsection{Page Segmentation Mode and OCR Engine}

Two variables, \code{pageSegMode} and \code{engineMode}, deserve
special mention because these two can have large impact on the results of the
OCR. The first, the Page Segmentation Mode, adjusts how Tesseract
recognizes lines of text in the image. By adjusting Page Segmentation
Mode, the user can specify if the text is present in many different
configurations, including (but not limited to) single block, sparse
text, vertical or circular arrangement, etc. The \pkg{Rtesseract}
package preserves the defaults in the \code{TesseractAPI} and assumes
the input is a single block of text (\code{PSM\_SINGLE\_BLOCK}) by
default, though in our experience OCR can be significantly improved
with changing the Page Segmentation Mode. We have found for most
problems \code{PSM\_AUTO}, which attempts to detect the correct page
segmentation automatically, is a good default, though any of the currently
\Sexpr{length(grep("^PSM_",ls('package:Rtesseract')))} available Page
Segmentation modes might be best for different applications (curiously,
\code{PSM\_AUTO} is the default for the tesseract command line tool
but not for the API). Page Segmentation Mode can be set either in the
call to \code{tesseract}, in the \code{...}  arguments for any of the
high-level functions (e.g., \code{GetText(f, pageSegMode =
"PSM\_AUTO")}), or by using \code{SetPageSegMode} on an existing
\code{TesseractBaseAPI} object.

The second of these variables is the Tesseract Engine Mode, which
determines what OCR engine is used to recognize text. By default,
\pkg{Rtesseract} uses Tesseract only. Depending on installed version
of Tesseract, other OCR engines can be used individually or in
combination with Tesseract (Cube for Tesseract versions 3.04+, LSTM for
Tesseract versions 4.00+). Although the use of these in combination
with Tesseract can increase accuracy, their use can add significant
computational load (between 6--10x) and require appropriately trained
language files, so they are not used by default by \pkg{Rtesseract}'s
OCR functions. 

\subsection{Image Pre-processing}

Consider the image previous image. When we perform OCR on this,
tesseract treats the highlighted text in "yellow" as a dark rectangular
block and ignores it.

Let's see what we get from `tesseract()` with this image:

<<>>=
t0 = tesseract(f)
w0 = GetText(t0, "RIL_TEXTLINE")
@

Some of the words in the highlighted text are `Drs. Dick and Haddow`.
Are these in the words returned by tesseract:

<<>>=
grep("Drs. Dick and Haddow", w0, value = TRUE)
@ 

Of course, there is text under this highlight.
We need to pre-process the image before we pass it to Tesseract to have the possibility of
recovering this text.
There are many options for pre-processing images, including
Imagemagick, OpenCV, or Leptonica. Leptonica is already used by
Tesseract to do some processing of the image, e.g., de-skew, remove
lines, etc. Since Leptonica is already interfaced with Tesseract
(i.e., Tesseract works off the \code{Pix} object created by
Leptonica), we include some bindings to the Leptonica library in the
\pkg{Rtesseract} package for basic image processing.

In this case, we need only convert the image from color to gray-scale or 8 bits per pixel.
We do this with the  `pixConvertTo8()` function.
So we read the image and convert it with

<<>>=
pix = pixRead("../images/SMITHBURN_1952_p3.png")
pix1 = pixConvertTo8(pix)
@

We can then pass the modified image  to the `tesseract()` function:

<<>>=
t1 = tesseract(pix1)
@

Then we call, e.g., `GetText()` as before, e.g.,

<<>>=
w1 = GetText(t1, "RIL_TEXTLINE")
@

We then check whether the words we see under the highlighted text are in the words
returned by tesseract for this modified image:

<<>>=
grep("Drs. Dick and Haddow", w1, value = TRUE)
@ 

Indeed all three are there.

\subsection{Using Leptonica to identify lines}

A primary motivation underlying the Rtesseract package is that we need
context from the rendered page to make sense of the results of the
OCR. Rather than just getting the text back, generally, we want
information about the location and size for each character, word or line identified by
tesseract so that we can use it to intelligently interpret the
content. This includes identifying document and section titles in the
document based on the size of the text.  In this example, we'll 
look at the locations of lines on the page in order to be able to
interpret data within tables.  Again, consider the previous image.

We can clearly see the data in TABLE 1 are arranged by row and column.
The column headers are separated by two horizontal lines spanning the width of the text on the page.
The final 4 columns have two column headers that span all 4 columns.
The header in column 4 (NO. STRAINS ISOLATED) is on 3 separate lines but
clearly differentiated from the other data rows in the table due to the
horizontal line separating the header from the data rows.

Firstly, the tesseract() function allows us to get the horizontal and
vertical locations of the words on the page. So we could use these to
attempt to recover the tabular nature of this data.  In this
particular example, we can do this effectively and relatively
simply. However, in images with tables that have center-aligned
columns (e.g. the "Year Isolated" column), this is much more
complicated and can lead to ambiguity regarding to which column a cell
belongs.  Instead, we can separate the values into their respective
columns much more effectively if we can identify the vertical lines
between columns.  Similarly, we can separate the column header cells
from the data cells below if we know the locations of the horizontal
line(s) that divide them.  Furthermore, we can determine the end of
the table and the continuation of the regular text below it if we can
find the final horizontal line in this example.

Given the motivation of finding the vertical and horizontal lines on
the page, let's explore how we can do this with Rtesseract.
Unfortunately, we have to do  this outside or separately from the  OCR
stage of recognizing the text.
The OCR process tesseract performs actually identifies
and removes the lines from the image before it recognizes the
remaining text on the image. This aids the accuracy of the OCR.
However, it does not allow us to query the locations of the
lines it removed.  Accordingly, we must replicate approximately the
same computations it performs to identify the lines and their
locations.  Fortunately, the basic ideas of this process are described
(here)[http://www.leptonica.com/line-removal.html]. The \pkg{Rtesseract}
package provides corresponding (and higher-level convenience)
functions to effect the same computations.

Note that we will pass the original/raw image to OCR without modifying
it. Tesseract will do many of the same computations we do. So our aim
here is to simply identify the locations of the horizontal and
vertical lines in the image before or after we perform the OCR.
 
We start by reading the image:
<<>>=
library(Rtesseract)
f = "../images/SMITHBURN_1952_p3.png"
p1 = pixRead(f)
@

We immediately convert the image to gray-scale with `pixConvertTo8()`:
<<>>=
p1 = pixConvertTo8(p1)
@

(Note that this creates a new Pix and allows the previous one to be garbage collected
as we assign the new image to the same R variable.)

The next step is to correct any skew or orientation in the image.
This can be important in detecting horizontal and vertical lines that may be at an angle
because the scan was done with a slight rotation.
There is a function deskew() to do this. However, we will show the steps
as they illustrate some of the leptonica functions available in the
Rtesseract package.

To detect the skew, we must first create a separate binary image using
`pixThresholdToBinary()`.
Then we call `pixFindSkew()` to determine the actual angle of the skew.
Then we rotate the original image
by that skew angle:

<<>>=
bin = pixThresholdToBinary(p1, 150)
angle = pixFindSkew(bin)
p2 = pixRotateAMGray(p1, angle[1]*pi/180, 255)
@

The 255 in the call to `pixRotateAMGray` corresponds to white
and means that any pixels that are "revealed" by rotating the
"page" are colored white. These occur in the corners.
We can specify any gray-scale color value we want here between 0 (black) and 255 (white).

It is useful to look at the resulting image:

<<fig = TRUE>>=
plot(p2)
@

With the reoriented image, we call the Rtesseract function
`getLines()` to get the coordinates of the lines in the image.
We'll first extract the horizontal lines.
For this, we pass the image and then desribe a "mask" or box
that helps to identify contiguous regions in the image corresponding
to the lines we want. For  horizontal lines, we want wide and vertically short boxes.
We describe this box with the width and height of the box, e.g., 

<<>>=
hlines = getLines(p2, 51, 3)
hlines
@

This returns a list with each element identifying the (x0, y0) and (x1, y1) coordinates
describing a single line. 

We can add these lines to the display of the image to verify they appear where
we expect:

<<>>=
plot(p2)
lapply(hlines,
        function(tmp) {
	   lines(tmp[c(1, 3)], nrow(p2) - tmp[c(2, 4)], col = "red", lty = 3, lwd = 2)
        })
@

Note that we had to subtract the y coordinates from the number of rows of the image
as R plots from bottom up and the coordinates for the lines are relative to
row 1 and go down.

getLines() performs several operations on the image.
It first uses the findLines() function
to return an image that contains either the horizontal or vertical lines.
We'll first get the horizontal lines with

<<<>>=
h = findLines(p2, 51, 3)
@

We specify the Pix to process and then the horizontal width
and vertical height (in pixels) that define a window or a region.
This fills in gaps in potential lines within the region.
Leptonica moves this around and fills in gaps within it if the
rest of the pixels within the region are not white.
findLines() uses the function pixCloseGray() to do this.

Currently, `findLines()` returns an image (Pix object).
By default, this shows the potential lines and removes everything else.
(We can also return the image with the lines removed and the text remaining.)

Let's look at the resulting image to see what lines it detected:

<<>>=
plot(h)
@

Note that the background is black, corresponding to values of 0 in the Pix.
We see the 4 primary lines that span most of the width of the image.
The second one down (at y = 3600) has several short gaps.
We also see many short line segments above the first line and to the right of the image.
The ones on the right correspond to parts of the letters in the rotated text
in the image indicating "Downloaded from http://www.....".
If we made the vertical height of our mask larger, we may discard these but at the risk
of including other elements.

`findLines()` returns a binary image containing the potential lines.
We convert this to a matrix of 0s and 1s in R:

<<>>=
m = pixGetPixels(h)
@

(or simply h[,])

We can see 4 long horizontal lines the original image that span the width of the text,
and we also see two shorter line segments above and below the text
"No. of strains isolated from".

We are looking for rows in the image that have many black pixels
(with value 1).
We find the rows of this matrix that have 1000 or more
black pixels.  We do this with
<<>>=
w = rowSums(m) > 1000
@

The value 1000 is context-specific.  It includes lines
that span 1000 pixels or more,  and also lines that have several smaller segments whose length adds up to 1000 pixels.
1000 is about 25% of the columns.  This might be too large to detect the
shorter line segments on the right of the table header, but it is probably
enough to ignore the small line segments that don't interest us.

How many rows of the matrix satisfy this criterion:

<<>>=
table(w)
@

So we have 46 which is a lot more than the 6 we see in the original image.

We can see these horizontal lines on the image with

<<>>=
plot(h)
abline(h = nrow(m) - which(w), col = "red")
@

Note again that the rows in the matrix go from 1 to nrow, while on the plot,
they go down the page. This is why we get the height with  nrow(m) - which(w).

We can see we get the 4 lines that span the width of the text area, but not the two shorter line segments.
So let's relax the threshold of requiring 1000 black pixels. We'll use 10% of the columns:

<<>>=
w = rowSums(m) > .1 * ncol(m)
abline(h = nrow(m) - which(w), col = "red")
@

So we detect these two shorter lines and no other additional lines (except the original 4 longer lines).
However, now we have 74 rows in the matrix that satisfy this criterion.
Yet we see only 6 lines on the plot.

Let's look at the row numbers for these 74 rows:
<<>>=
which(w)
@

It may not be clear, but these are grouped together.  Each of the red
lines we see on the plot is actually a collection of multiple adjacent
lines.  This is clearer if we compute the difference between the row
numbers:

<<>>=
diff(which(w))
@

We see a collection of 1s meaning that these are adjacent rows, then a large
jump between row numbers, followed by a collection of 1s and on.
So it is clear these are grouped.
We want to combine adjacent lines into a single group
and have a group for each conceptually separate line on the image.
One way to do this is run length encoding (RLE):

<<>>=
rle(diff(which(w)))
@

Another way is very similar but more explicit and gives us a little more control.
We find where the differences between successive row numbers is greater than 2
and then we run a cumulative sum which gives us group labels:

<<>>=
g = cumsum( diff(which(w)) > 2 )
@

We can use g to split the rows into groups

<<>>=
rowNums = which(w)
ll = tapply(seq(along = rowNums), c(0, g), function(i) m[rowNums[i], ])
names(ll) = tapply(seq(along = rowNums), c(0, g), function(i) as.integer(mean(rowNums[i])))
@

(Note that we added 0 to the start of the vector g since g has one less element than
w since we were computing pairwise differences.)
The computation is a little akward. We split the indices of rowNums and then have
to use those to index back into rowNums to find out which rows of m we need to extract for this group.

The third line puts the average of the row numbers of each group as
the name of that element.  This will be convenient for mapping this
back to the general vertical area.

Each element of `ll` is a matrix. For each of these, we  want to combine them
to identify the start and end of a line. Each may have several separate line segments.

Lets consider the first element of `ll`. It has 14 rows and 4050 columns.
It is useful to query how many 0s and 1s there are?

<<>>=
table(ll[[1]])
@

These are about equal:  27000 0s and 30000 1s.
(In this setup, 1 is black.)
So there are many white pixels eventhough the black pixels appear as solid line on our plot.
But that was because we just drew a straight line at these vertical positions, regardless how many pixels were white. Of course, we selected these rows because they had many black pixels.

Since we think these rows make up a single line, let's aggregate across the adjacent
rows to form a single aggregate row using colSums():

<<>>=
rr = colSums(ll[[1]])
@

If this is a solid line across most of the page, we'd expect most pixels to be filled,
perhaps with a few short gaps.
Each element of rr contains the number of rows in this group that had a black pixel.
This might be 1 or 14, with only one row or all of them having a black pixel.
We might look at these counts or just consider whether any row has a black pixel in this column. We are looking for a rule to mark a pixel in the aggregated line as black or white.

If we look at the vector rr, we see a lot of 0s at the beginning and the end. These are the margins where the line is not present.

If we want to threshold this aggregate line to have a black pixel in a column
if *any* of the pixels in the adjacent rows defininig it
has a black pixel, we can do this with

<<>>=
rle(rr > 0)
@

We get the two margins and a long run of 2895 pixels.
If we require at least half of the 14 rows to have a black pixel in a column,
we get the same groups:

<<>>=
rle(rr >= 7)
@

Let's consider one of the shorter lines.
These are elements 2 and 3 of ll. 
Again, we compute the column sums for the adjacent lines in this group:

<<>>=
rr = colSums(ll[[2]])
table(rr)
@

Most are 0.
There are 9 adjacent rows in this group.
We see no columns that have at least one black pixel but less than half the number of rows.
Let's draw points at each of the columns that have a black pixel in at least half the rows:

<<>>=
z = which(rr >= 4.5)
points(z, nrow(m) - rep(as.integer(names(ll)[2]), length(z)), col = "green", pch = 16)
@

This appears to capture the short horizontal line spanning the four columns
on the right of the table. So this appears to be a reasonable approach,
at least for this image.

\subsubsection{Vertical Lines}

We can follow the same process to identify the
locations of the vertical lines. We've already deskewed the original
image, so we don't have to do that.  We can call findLines() and this
time specify a window that is tall and narrow to capture the vertical
parts.  We should experiment with these values.  One thing to note is
that characters have vertical components more than horizontal
components, e.g. 1, L, M.  If we make the height of the window too
small, we'll pick up more false positives.

<<>>=
v = findLines(p2, 3, 101)
@

Again, we can plot this and see where the lines are.

\subsubsection{Removing the Lines}

We don't have to remove the lines before we pass the image to tesseract
as tesseract will remove them for us.
However, it is interesting to see how well we can do.
Instead of finding the locations of the lines (with getLines()),
we'll remove them from the image.
This uses the same basic approach and sequence of steps.
However, rather than extract the line coordinates,
we use `pixAddGray` to overlay the images with the horizontal and then
the vertical lines onto the original image.
This removes the lines from the original image.
We do this directly from the image file with

<<>>=
p1 = pixRead(f)
p2 = pixConvertTo8(p1)
p2 = deskew(p2)
p3 = findLines(p2, 51, 1, FALSE)
p4 = findLines(p2, 3, 101, FALSE)
p = pixAddGray(p2, p3)
p = pixAddGray(p, p4)
plot(p)
@

Let's compare this to the original image.

<<>>=
plot(p2)
@

The horizontal and vertical lines are essentially gone. 
We can certainly see light grey "smudges" in some places where the vertical lines were.
We also see some short horizontal line segments remaining.

We can experiment with the horizontal and vertical values we pass to findLines().
Increasing the horizontal width from 51 to 75 cleans up the horizontal lines.

We can also change the thresholds for creating the binary images.
We can also threshold the final image (p) at this point.
We set any value 50 or above to 255, i.e., white, with

<<>>=
pp = pixThresholdToValue(p, 50, 255)
@

and when we plot the resulting image

<<>>=
plot(pp)
@

The gray marks on the vertical lines have disappeared.
Note however that it removed some of the text, specifically the rotated text on the right
and the URL within that text that is colored blue in the original image.
Fortunately, we don't care about this text for our application. 

We can compare the image we created with the lines removed to
the one tesseract creates.  We do this with


<<>>=
tesseract(f, pageSegMode = "psm_auto", textord_tabfind_show_vlines = 1)
@

This creates a PDF file named vhlinefinding.pdf.

\subsection{Other Variables}

There are a plethora of variables that can be adjusted, so for space
considerations we will highlight the ones we have found the most
useful in our work. To continue the example above, on inspection of
the results of the OCR there are some domain-specific terms that are
not being recognized correctly. While it might be possible to correct
these in post-processing, it is preferable to tweak the settings of
the OCR engine to allow the correct recognition in the processing
stage. The above document includes the two terms that are consistently
mis-recognized; {\it{``Ae. aegypti''}} (a scientific name) and ``São
Paulo'' (a non-English place name). \code{Tesseract} allows
user-specified words to be added to the dictionary of correct matches
through a ``user-words'' file. \pkg{Rtesseract} allows you to specify
this file, which will then be used by the OCR engine to resolve
ambiguities:

<<new>>=
# Create a user-words file for English
writeLines(c("2\tz'\t1\t1\0"), "eng.unicharambigs")
writeLines(c("Zika", "Anopheles"), "eng.user-words")

# Create a new API for comparison
api2 = tesseract(f, pageSegMode="psm_auto",# engineMode=2,
                 # datapath = path.expand("~/DSI/tessdata/"))#,
                 opts = list(user_words_file = "eng.user-words",
                             tessedit_enable_dict_correction = "1"))


#Fixed error in OCR engine rather than post-process results
grep("^An", GetText(api2), value = TRUE)
GetText(api, "textline")[2]

@

Extending this idea, we can explore which characters have the lowest
confidence and/or occurrence in the text.

<<>>=
symbolBB = GetBoxes(api, level = "symbol")
symbolConf = sort(tapply(symbolBB[,"confidence"], row.names(symbolBB), median))
# Lowest eight 
symbolConf[1:8]
@

We can visually inspect these low-confidence characters to verify that
they are correct using the \code{plotSubImage} function for a single
bounding box, or the \code{plotSubsets} function to create a panel
plot of multiple bounding boxes (Fig. \ref{fig:subsets}.

\begin{figure}
<<fig = TRUE>>=
idx = which(symbolBB[,"confidence"] < 70)

plotSubsets(symbolBB[idx,], img = png::readPNG(f))
@
\caption{An example of the \code{plotSubsets} function, which creates panel plots of
  individual subsets of the image along with the associated OCR result
  and confidence. In this example, the ``ã'' character is oft
  mis-recognized.}
\label{fig:subsets}
\end{figure}

Continuing the example, say we were primarily interested in the
abstract. Using the bounding boxes or other coordinates as guides, we
are able to adjust the OCR-ed area without leaving R. Typically, to do
this would involve partitioning the image into subsets using an image
editing tool (e.g., Gimp, Darktable, Photoshop), and then running the
OCR on each of these subset images. This process involves leaving R
and involves steps that may be difficult to replicate. However,
\pkg{Rtesseract} allows the user to specify a rectangle of interest
using the \code{SetRectangle} function. This function takes a
\code{TesseractBaseAPI} object and the left, top, width and height of
the sub-area as additional arguments (the dimensions of the full input
image can be found with \code{GetImageDims}). Once set, the next call to the
\code{TesseractBaseAPI} will only recognize text within this rectangle
- no external image manipulation required. Since we know that the
abstract is between the ``Abstract'' header and the ``Keywords''
tagline, we can easily find the coordinates of these using the
bounding boxes and then only recognize text within this area:

<<>>=
bb = GetBoxes(api)
idx = grep("Abstract|Keywords", rownames(bb))
bb[idx,"top"]
GetImageDims(api)

SetRectangle(api, 0, 950, 4000, 1800 - 950)
GetText(api, level = "textline")

#Reset rectangle
GetImageDims(api)

SetRectangle(api, 0, 0, 4013, 3210)
@ 

Many customizable variables which allow fine control of the OCR engine
are accessible through R via the \pkg{Rtesseract} package, including
pageSegMode, pageOrientation, language, white- and black-list
characters, custom user-added patterns and words, alternative OCR
engines, etc. The current settings for the tesseract OCR engine are

%% Treat the API as a list, names(api) 
returned by \code{PrintVariables} (returns
\Sexpr{length(PrintVariables())} variables at the time of
writing). These variables can be adjusted via a call to
\code{SetVariables}.  Full documentation of user-adjustable variables
is available at
\url{https://github.com/tesseract-ocr/tesseract/blob/master/doc/tesseract.1.asc}.

Tesseract requires trained language files in order to operate. 
By default, \pkg{Rtesseract} searches the /usr/bin/local directory for
installed trained language files, but this can be overridden. The
current location where the API is searching for language data is
returned by \code{GetDataPath}. Since Tesseract allows users to create
trained data files for custom languages, the ability to specify the
path of the language files alleviates the need to place these language
data files in the default search location. More information on
training Tesseract and providing language files can be found at
\url{https://github.com/tesseract-ocr/tesseract/wiki/Training-Tesseract}. 

%XXX This seems a little arcane and not necessarily relevant for this
%article, i.e. not necessarily % correct level.
% ME - I think it is OK for this paper

Lastly, we have found that some experimentation is required to settle
on the best set of customized variables for more difficult cases. By
creating persistent TesseractBaseAPI objects via the \code{tesseract}
function, two or more \code{TesseractBaseAPI}s can be created for the
same image, which allows you to compare the impact of various settings
to fine tune the OCR for individual cases. Once a suitable set of
adjustments have been found, these settings can be exported using
\code{PrintVariables}. These custom variables can then be easily set
using \code{SetVariables} or the \code{opts} argument in the
\code{tesseract} function.

% Looking at and changing options
<<>>=
#Querying the adjustable variables
v = PrintVariables()
v[grep("tessedit_char", names(v))]

@ 

\subsection{Writing directly to file}

\pkg{Rtesseract} includes functionality to export the results of OCR
to many conventional file types, including BoxText, HOcr/HTML , OSD
(Orientation and Script Detection), TSV (tab separated values), and
PDF. Of these, \code{toPDF} is unique in that it creates a searchable
PDF document, not a plain-text document.

\subsection{Metadata}

Lastly, \pkg{Rtesseract} allows users to access metadata about the OCR
engine and the input image. The current version of Tesseract can be
returned by \code{tesseractVersion}, while the capabilities for
reading various image formats via leptonica is provided by
\code{leptonicaImageFormats}. Information about the source image are
also accessible through R using the \code{GetInputName},
\code{GetImageInfo},\code{GetImageDims}, and
\code{GetSourceYResolution} functions.

\section{Installation}

In order to use \pkg{Rtesseract}, users must first have both Tesseract
and its dependency, Leptonica, installed. Tesseract is available from
\url{https://github.com/tesseract-ocr/} and Leptonica from
\url{https://github.com/DanBloomberg/leptonica}. Pre-compiled binaries
are available for most platforms for the current stable version
(v3.05). As previously mentioned \pkg{Rtesseract} also works with the
development version of Tesseract (4.00alpha) currently available from
\url{https://github.com/tesseract-ocr/tesseract/releases}. However, to
build Tesseract from source requires many additional pieces of
software, including leptonica v1.74+, automake, libtool, pkg-config,
aclocal, autoheader, and autoconf-archive. Additionally, users will
also need to install the training data for the language(s) used in the
documents to be processed.  Currently, there are pre-trained data sets
for 140+ languages available from
\url{https://github.com/tesseract-ocr}. Users can also provide their
own trained data. For more information about training Tesseract,
please see
\url{https://github.com/tesseract-ocr/tesseract/wiki/Training-Tesseract}. \pkg{Rtesseract}
includes some functions that assist with training, including
\code{ReadBoxFile()}. Finally, the \pkg{Rtesseract} package itself can
be installed from source at
\url{https://github.com/duncantl/Rtesseract}.

% Different section title
\section{Conclusion}

\pkg{Rtesseract} provides both easy access to basic OCR functionality
for simple use-cases as well as access to low-level control parameters
for more advanced uses. By allowing recovery of additional information
generated by Tesseract, such as confidence levels and bounding boxes,
\pkg{Rtesseract} supports novel applications of inputting data from
images into R (e.g., spatial analysis of text). Leveraging free,
open-source software allows full integration of the work-flow of
text from images into R.

\bibliography{Rtesseract.bib}
